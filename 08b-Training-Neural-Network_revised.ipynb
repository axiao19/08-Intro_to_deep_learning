{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a CNN\n",
    "Today we're going to take the datasets we created yesterday and train a convolutional neural network (CNN) on them. Since this directly depends on the output files from yesterday, take a moment to copy and paste your output files into today's directory. Again, let's take a second to remember our two guiding questions:\n",
    "\n",
    "- _What_ is in an image (e.g. debris, buildings, etc.)?\n",
    "- _Where_ are these things located _in 3D space_ ?\n",
    "\n",
    "Recall that we are posing the first question as a classification problem (e.g. does this image have flooding or not), and we are deploying a CNN to aid in the classification.\n",
    "\n",
    "As before, this is heavily based off of the following tutorials: https://github.com/LADI-Dataset/ladi-tutorial/blob/master/Tutorials/Pytorch_Data_Load.md, \n",
    "\n",
    "https://github.com/LADI-Dataset/ladi-tutorial/blob/master/Tutorials/Train_Test_Classifier.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "Yesterday we developed simple machine learning models using sci-kit learn. While the package is good for quick and dirty machine learning models, we are going to need much more powerful tools to take on an image classification task of the kind we're seeing. To that end, we're going to be working with PyTorch, typically considered as the state of the art in deep learning. It was developed primarily by Facebook AI in and released for Python in 2017. There are two main advantages of PyTorch vs. many other Python packages:\n",
    "- A fairly high level interface for building neural networks\n",
    "- Parallelization support via GPU\n",
    "While it is one of the easier ways to build a neural network, there is still a fairly steep learning curve. Thus, today will be all about learning how to implement in PyTorch (and less about what's going on under the hood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # turn off warnings\n",
    "# warnings.filterwarnings('default') # regular level warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset from your csv files\n",
    "The first step to using PyTorch is creating a dataset. All datasets in PyTorch need to be a subclass of torch.utils.data.Dataset, which is the general class that represents a dataset. When creating your dataset, you usually need to overwrite two methods:\n",
    "- ```__len__```, so that when you call len(dataset) it returns the number of images\n",
    "- ```__getitem__```, so that when you call dataset[i] it returns the ith item. \n",
    "As usual, we're also going to define ```__init__``` to take in the arguments we care about.\n",
    "\n",
    "We also need to decide what we want a sample of our dataset to look like. Ours is going to look like a dictionary as follows: ```{'image': image, 'image_name': img_name, 'damage:flood/water': label, uuid': uuid, 'timestamp': timestamp, 'gps_lat': gps_lat, 'gps_lon': gps_lon, 'gps_alt': gps_alt, 'orig_file_size': file_size, 'orig_width': width, 'orig_height': height}```. Of course, there's no requirement that it be a dictionary, though it should somehow incorporate the input and output (in our case, the image and the label). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure GPU is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convenient function for showing the images\n",
    "def show_image(image):\n",
    "    plt.imshow(image)\n",
    "    # pause a bit so that plots are updated\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LadiDataset(Dataset):\n",
    "    def __init__(self, label_csv, class_names, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            label_csv (str): path to csv with columns for url, local_path, and columns for each label class\n",
    "            class_names (list[str]): list of the column names corresponding to each label class\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.label_data_df = pd.read_csv(label_csv)\n",
    "        self.class_names = class_names\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        ## Load images from local directory. There is no need to redownload images to local machine. ##\n",
    "        local_path = self.label_data_df.iloc[idx]['local_path']\n",
    "        url = self.label_data_df.iloc[idx]['url']\n",
    "        try:\n",
    "            image = Image.fromarray(io.imread(local_path))\n",
    "            img_name = local_path\n",
    "        except:\n",
    "            image = Image.fromarray(io.imread(url))\n",
    "            img_name = url\n",
    "        label = torch.tensor(self.label_data_df.iloc[idx].loc[self.class_names].values.astype(float))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        example = {'image': image, \n",
    "                   'image_name': img_name, \n",
    "                   'label': label}\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = ['damage:flood/water','damage:rubble','damage:misc']\n",
    "# Let's do a quick sanity check\n",
    "damage_dataset = LadiDataset('damage_examples.csv', classes)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i, sample in enumerate(damage_dataset):\n",
    "\n",
    "    print(i, \n",
    "          sample['label'], \n",
    "          sample['image_name'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title('Sample #{}'.format(i))\n",
    "    show_image(sample['image'])\n",
    "\n",
    "    if i == 5:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "There are now two things to take care of:\n",
    "- Neural networks usually expect all images to be of the same size, but sometimes datasets are not all of the same size. The LADI dataset includes image of different sizes. \n",
    "- Like we saw in our thought exercise last class, there is a huge amount of variability. Because of this, the top performing CNN's usually train on hundreds of thousands of images, much more than what we have! We're going to have to somehow increase the number of images available to us. \n",
    "\n",
    "One solution is to augment the dataset by *transforming* it. Most transformations are actually fairly straightforward.\n",
    "\n",
    "- ```Resize```: to resize the input PIL Image to the given size.\n",
    "- ```RandomCrop```: to crop from image randomly. This is data augmentation.\n",
    "- ```RandomRotation```: to rotate the image by angle.\n",
    "- ```RandomHorizontalFlip```: to horizontally flip the given PIL Image randomly with a given probability.\n",
    "- ```ToTensor```: to convert the numpy images to torch images (we need to swap axes).\n",
    "\n",
    "These functions each have their own set of arguments, which can be referenced here(https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "Now let's try to apply these transforms, first individually and then all at once. The key to effective transforms is to have \"enough\" transformation that the input image is visually distinct from the output image, but not so much that the output image is no longer representative of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flip = transforms.RandomHorizontalFlip(p=0.5) # some probability of flipping\n",
    "scale = transforms.Resize(1024) # resize to fixed size\n",
    "affine = transforms.RandomAffine(degrees=20, translate=(0.15, 0.15), scale=(0.8,1.6)) # affine transform\n",
    "jitter = transforms.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.02) # add color jitter noise\n",
    "perspective = transforms.RandomPerspective(0.3, 0.5) # random perspective changes\n",
    "c_crop = transforms.CenterCrop(512) # crop center 512x512 pixels\n",
    "\n",
    "flip_demo = transforms.RandomHorizontalFlip(1) # flip with 100% chance just to demo\n",
    "composed_demo = transforms.Compose([scale,\n",
    "                               affine,\n",
    "                               perspective,\n",
    "                               jitter,\n",
    "                               c_crop,\n",
    "                              flip_demo])\n",
    "\n",
    "# Apply each of the above transforms on sample.\n",
    "\n",
    "sample = damage_dataset[190]\n",
    "for i, tsfrm in enumerate([scale, c_crop, affine, flip_demo, jitter, perspective, composed_demo]):\n",
    "    transformed_image = tsfrm(sample['image'])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tsfrm).__name__)\n",
    "    show_image(transformed_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Compared to simple ```for``` loop to iterate over data, ```torch.utils.data.DataLoader``` is an iterator which provides more features:\n",
    "\n",
    "- Batching the data.\n",
    "- Shuffling the data.\n",
    "- Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "In the previous section, three transforms are performed on a sample. In this section, users can learn to use ```DataLoader``` to transform all images in the dataset.\n",
    "\n",
    "First, a new dataset with transform needs to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for training, we're adding the convert to tensor transform\n",
    "composed = transforms.Compose([scale,\n",
    "                               affine,\n",
    "                               perspective,\n",
    "                               jitter,\n",
    "                               c_crop,\n",
    "                              flip,\n",
    "                              transforms.ToTensor()])\n",
    "transformed_dataset = LadiDataset('damage_examples.csv', classes, composed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataloader to load the data for illustrative purposes\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=8,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to show a batch\n",
    "def show_images_batch(sample_batched):\n",
    "    images_batch = sample_batched['image']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "    grid_border_size = 2\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.title('Batch from dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].cuda().size())\n",
    "\n",
    "    # observe 1st batch and stop.\n",
    "    if i_batch == 0:\n",
    "        plt.figure()\n",
    "        show_images_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using the csv files you created yesterday, create your own DataLoader using transforms that you think would be interesting or helpful. Here's a full list of transforms: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "As a group, answer the following questions\n",
    "- Are the image labels accurate? That is to say, if it is labeled as having a flood, does it actually have a flood?\n",
    "- Do the transforms add enough variety to the dataset? Do they still look representative of the dataset?\n",
    "- Do the transforms distort from the labels? For example, if you use a random crop, is the thing that is being labeled still present in the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network\n",
    "We are now ready to train a CNN! We will be using a pre-trained model as our starting point to reduce the amount of training that we need to do. The `torchvision` module includes common pretrained models. We will be using one called `resnet50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True # flag for some GPU optimizations\n",
    "torch.hub.set_dir('pytorch-hub-cache') # set location for saving pretrained models\n",
    "net = torchvision.models.get_model('resnet50', pretrained=True) # load pretrained model\n",
    "\n",
    "# pretrained neural network is missing the last layer, we add it back on\n",
    "net.fc = nn.Linear(2048, 3) # resnet50 create 2048 features in its first half, we have 3 classes\n",
    "net = nn.DataParallel(net) # specify the data distribution strategy if we have more than one GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # send it to GPU if it's available\n",
    "net.to(device) \n",
    "print(\"ResNet ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to take a familiar step: we're going to split the dataset into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_test_loaders(dataset, test_split_ratio=0.2, batch_size=4, shuffle_dataset=True, random_seed=0, num_workers=1):\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(test_split_ratio * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                               sampler=test_sampler, num_workers=num_workers)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "damage_train_loader, damage_test_loader = get_train_test_loaders(transformed_dataset, batch_size=8, random_seed=42, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to define the loss function and the optimizer. You can think of the loss function as the amount of error you want to minimize, while the optimizer is the specific tool you're going to use to minimize that loss. \n",
    "\n",
    "Because the data is not evenly represented, we want to re-weight each class based on how many examples there are in the dataset. This way we don't get biased toward more commonly represented classes in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "damage_df = pd.read_csv('damage_examples.csv')\n",
    "\n",
    "# parameters for Damage Model\n",
    "flood_weight = (len(damage_df)-damage_df['damage:flood/water'].sum())/damage_df['damage:flood/water'].sum()\n",
    "rubble_weight = (len(damage_df)-damage_df[\"damage:rubble\"].sum())/damage_df[\"damage:rubble\"].sum()\n",
    "misc_weight = (len(damage_df)-damage_df[\"damage:misc\"].sum())/damage_df[\"damage:misc\"].sum()\n",
    "pos_weight = torch.as_tensor([flood_weight, rubble_weight, misc_weight], dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to do the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(net, train_loader, test_loader, criterion, optimizer, scheduler, logs_path, model_name, \n",
    "                starting_epoch=0, additional_epochs=10, print_every_num_batches=100):\n",
    "    model_name_base = f'resnet50-{model_name}'+'.ep{}.pth'\n",
    "    writer = SummaryWriter(logs_path)\n",
    "    checkpoints_path = logs_path/'checkpoints'\n",
    "    checkpoints_path.mkdir(parents=True, exist_ok=True)\n",
    "    if starting_epoch > 0:\n",
    "        starting_epoch_string = str(starting_epoch).zfill(3)\n",
    "        model_load_path = checkpoints_path/model_name_base.format(starting_epoch_string)\n",
    "        net.load_state_dict(torch.load(model_load_path))\n",
    "    for epoch in range(starting_epoch, starting_epoch+additional_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_epoch_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data['image'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_epoch_loss += loss.item()\n",
    "            if (i+1) % print_every_num_batches == 0:    # print every 10 mini-batches\n",
    "                print(f'[epoch {epoch+1}, batch {i +1} ] average loss: {running_loss/print_every_num_batches}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        average_epoch_loss = running_epoch_loss/(i+1)\n",
    "        writer.add_scalar('Loss/epoch_avg/train', average_epoch_loss, epoch)\n",
    "        print(f'[epoch {epoch+1}] average training epoch loss: {average_epoch_loss}')\n",
    "        writer.add_scalar('LR/rate', scheduler.get_last_lr()[0], epoch)\n",
    "        scheduler.step()\n",
    "        running_epoch_loss = 0.0\n",
    "        print(\"Getting epoch test loss...\")\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data['image'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_epoch_loss += loss.item()\n",
    "\n",
    "        average_epoch_loss = running_epoch_loss/(i+1)\n",
    "        writer.add_scalar('Loss/epoch_avg/test', average_epoch_loss, epoch)\n",
    "        print(f'[epoch {epoch+1}] average test epoch loss: {average_epoch_loss}')\n",
    "        epoch_string = str(epoch+1).zfill(3)\n",
    "        model_save_path = checkpoints_path/model_name_base.format(epoch_string)\n",
    "        torch.save(net.state_dict(), model_save_path)\n",
    "\n",
    "    print('Finished Training')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = pathlib.Path('outputs')\n",
    "outputs.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_name = 'damage_model'\n",
    "\n",
    "# train damage model\n",
    "train_model(net, damage_train_loader, damage_test_loader, criterion, optimizer, scheduler, outputs, model_name, \n",
    "                starting_epoch=0, additional_epochs=5, print_every_num_batches=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using your own dataset, set up your neural network so that it is ready to train. Leave it training overnight (it will take a *long* time for it to run). Make sure there are no obvious errors in your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Now that we have a fully trained CNN, we would like to see how well it performs on unseen data. Let's first take a sample of testing images and displaying their ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(damage_test_loader)\n",
    "single_iter = next(dataiter)\n",
    "images = single_iter['image']\n",
    "labels = single_iter['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print images and ground truth\n",
    "imshow(utils.make_grid(images))\n",
    "print('GroundTruth: \\n', ' '.join('%5s' % labels[j].cpu() for j in range(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can load the saved model and make some predictions on the images above.\n",
    "Note that `True==1` and `False==0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('outputs/checkpoints/resnet50-damage_model.ep004.pth'))\n",
    "\n",
    "outputs = net(images.cuda())\n",
    "_, predicted = torch.max(outputs, 1) #gets the index of the max prediction, 0 = flood, 1 = rubble, 2 = misc\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % predicted[j].cpu()\n",
    "                              for j in range(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at how the trained network performs on the whole testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in damage_test_loader:\n",
    "        images = data['image'].cuda()\n",
    "        _, labels = torch.max(data['label'].cuda(),1)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can look at the performance of the model on each class of ```damage:flood/water: True``` and ```damage:flood/water: False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "truth_labels = []\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for data in damage_test_loader:\n",
    "        images = data['image'].cuda()\n",
    "        _, labels = torch.max(data['label'].cuda(),1)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        truth_labels.append(labels.cpu())\n",
    "        predicted_labels.append(predicted.cpu())\n",
    "truth_labels = np.concatenate([x.numpy() for x in truth_labels])\n",
    "predicted_labels = np.concatenate([x.numpy() for x in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(truth_labels, predicted_labels, labels=[0,1,2])\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=['flood', 'rubble', 'misc'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize only flood vs no flood (rubble/misc)\n",
    "confusion_matrix = sklearn.metrics.multilabel_confusion_matrix(truth_labels, predicted_labels, labels=[0,1,2])\n",
    "                    # outputs 3 matrices (one for each class of 'flood', 'rubble', 'misc'), with format\n",
    "                    # [[True Negatives (TN), False Positives (FP)],\n",
    "                    # [False Negatives (FN), True Positives (TP)]]\n",
    "                    # calculated using one-vs-rest label binarization\n",
    "# use index 0 to get the confusion matrix for the flood vs no-flood counts\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix[0], display_labels=['no flood', 'flood'])                    \n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Display the performance of your neural network. Is the overall accuracy as good as you expected? What about the class accuracy? Does it perform better on one class than another? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and ethics in machine learning\n",
    "While we wait for our neural networks to train, let's take a step back and think about what it is that we're trying to do. AI actually provides a very timely and rich context in which we can discuss questions of ethics, equity and justice. \n",
    "\n",
    "Background material:\n",
    "Designing AI with Justice (this will be our main piece), Costanza-Chock: https://www.publicbooks.org/designing-ai-with-justice/\n",
    "\n",
    "Can we make artificial intelligence ethical?, Schwarzman: https://www.washingtonpost.com/opinions/2019/01/23/can-we-make-artificial-intelligence-ethical/\n",
    "\n",
    "The Apple Card Didn't 'See' Gender—and That's the Problem, Knight: https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/\n",
    "\n",
    "Color film was built for white people. Here's what it did to dark skin, Vox: https://www.youtube.com/watch?v=d16LNHIEJzs\n",
    "\n",
    "## Science as a social process\n",
    "Something that might seem a bit strange and perhaps even a bit uncomfortable is the idea of science as a social construction. There's a couple of things to note here:\n",
    "- **Not everyone subscribes to this view of science.** There are a lot of more classical schools of thought that hold science as a sort of bastion of everything that is objective.\n",
    "- **Just because science is socially constructed does not mean that it does not exist.** Can you think of something else that exists even though it is a social construct?\n",
    "\n",
    "How are some ways that the social construction of science can present itself?\n",
    "- Who funds research and why? Under what constraints?\n",
    "- For whom is the product of research directed to?\n",
    "- Who creates the products?\n",
    "- Who holds (or has historically held) a position of position of power in society?\n",
    "- To what end is knowledge being generated?\n",
    "\n",
    "**How are some ways that human bias can seep into machine learning algorithms?**\n",
    "\n",
    "**How does this apply to the aerial imagery we are working with?**\n",
    "\n",
    "**What do we do about it? Should we just give up on creating these models?**\n",
    "\n",
    "### Exercise\n",
    "Take a critical look at both the LADI dataset and the techniques that we are applying to it. As a group, answer the following questions:\n",
    "- How might human bias be present in the LADI dataset? Think about both the imagery and the labels, as well as how these data were gathered?\n",
    "- How might human bias be present in the machine learning techniques we apply to these imagery?\n",
    "- If we were to build a classifier that can predict your choice of target, how might the problems outlined above yield issues in equity?\n",
    "- To what extent can these issues be resolved with the dataset and machine learning techniques as they exist?\n",
    "- If you could completely redo the process of arriving at the LADI dataset and doing your analysis, what would you do differently?\n",
    "\n",
    "Be prepared to present on these at the end of the discussion period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
